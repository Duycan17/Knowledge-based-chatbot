# Knowledge Base API

Há»‡ thá»‘ng knowledge base containerized sá»­ dá»¥ng FastAPI, Neon PostgreSQL vá»›i pgvector extension, LangChain vÃ  Gemini API. Há»— trá»£ upload file text vÃ  RAG (Retrieval-Augmented Generation).

## ğŸš€ Features

- **File Upload**: Upload vÃ  process file text (.txt, .md, .csv, .json, .pdf)
- **Vector Search**: Similarity search vá»›i pgvector
- **RAG Pipeline**: Retrieval-Augmented Generation vá»›i Gemini
- **Audit Logging**: Log táº¥t cáº£ chat interactions
- **Async Processing**: Background processing cho file uploads
- **Document Management**: XÃ³a documents vÃ  chunks tá»« cáº£ database vÃ  file system
- **Batch Operations**: XÃ³a nhiá»u documents cÃ¹ng lÃºc
- **Docker Support**: Containerized vá»›i docker-compose

## ğŸ› ï¸ Tech Stack

- **Backend**: FastAPI vá»›i async I/O
- **Database**: Neon PostgreSQL vá»›i pgvector extension
- **AI/ML**: LangChain, Gemini API
- **Container**: Docker vá»›i docker-compose
- **Caching**: Redis (optional)

## ğŸ“‹ Requirements

- Python 3.11+
- Docker & Docker Compose
- Neon PostgreSQL account
- Google AI API key

## ğŸš€ Quick Start

### 1. Clone Repository
```bash
git clone <repository-url>
cd knowledge-base-api
```

### 2. Environment Variables
Táº¡o file `.env`:
```bash
# Neon PostgreSQL
NEON_DATABASE_URL=postgresql://user:password@host:port/database

# Google AI
GOOGLE_API_KEY=your_google_api_key

# Optional Redis
REDIS_URL=redis://redis:6379

# File Processing
MAX_FILE_SIZE=10485760  # 10MB
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

### 3. Run with Docker
```bash
# Build vÃ  start services
docker-compose up --build

# Run in background
docker-compose up -d
```

### 4. Run Locally
```bash
# Install dependencies
pip install -r requirements.txt

# Run application
python main.py
```

## ğŸ“š API Documentation

### File Upload & Knowledge Management

#### POST /knowledge/upload
Upload file text vÃ  process thÃ nh embeddings.

**Request:**
```bash
curl -X POST "http://localhost:8000/knowledge/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.txt"

# Hoáº·c upload PDF
curl -X POST "http://localhost:8000/knowledge/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf"
```

**Response:**
```json
{
  "file_id": "uuid",
  "filename": "document.txt",
  "status": "processing",
  "message": "File uploaded successfully and processing started",
  "file_size": 1024
}
```

#### POST /knowledge/upload/batch
Upload nhiá»u file text cÃ¹ng lÃºc vÃ  process thÃ nh embeddings.

**Request:**
```bash
curl -X POST "http://localhost:8000/knowledge/upload/batch" \
  -H "Content-Type: multipart/form-data" \
  -F "files=@document1.txt" \
  -F "files=@document2.txt" \
  -F "files=@document3.txt"
```

**Response:**
```json
{
  "message": "Batch upload completed. 2 successful, 1 failed",
  "total_files": 3,
  "successful_uploads": 2,
  "failed_uploads": 1,
  "uploads": [
    {
      "file_id": "uuid-1",
      "filename": "document1.txt",
      "status": "processing",
      "message": "File uploaded successfully and processing started",
      "file_size": 1024
    },
    {
      "file_id": "uuid-2",
      "filename": "document2.txt",
      "status": "processing",
      "message": "File uploaded successfully and processing started",
      "file_size": 2048
    }
  ],
  "errors": [
    {
      "filename": "document3.txt",
      "error": "File size too large"
    }
  ]
}
```

#### GET /knowledge
Láº¥y danh sÃ¡ch documents vá»›i pagination.

#### DELETE /knowledge/{doc_id}
XÃ³a document vÃ  táº¥t cáº£ chunks cá»§a nÃ³ tá»« cáº£ database vÃ  file system.

**Request:**
```bash
curl -X DELETE "http://localhost:8000/knowledge/{doc_id}"
```

**Response:**
```json
{
  "message": "Document deleted successfully"
}
```

#### DELETE /knowledge/batch
XÃ³a nhiá»u documents cÃ¹ng lÃºc.

**Request:**
```bash
curl -X DELETE "http://localhost:8000/knowledge/batch" \
  -H "Content-Type: application/json" \
  -d '["doc_id_1", "doc_id_2", "doc_id_3"]'
```

**Response:**
```json
{
  "message": "Batch delete completed. 2 successful, 1 failed",
  "results": {
    "successful": ["doc_id_1", "doc_id_2"],
    "failed": ["doc_id_3"],
    "total_requested": 3,
    "total_successful": 2,
    "total_failed": 1
  }
}
```

**Request:**
```bash
curl "http://localhost:8000/knowledge?page=1&size=10"
```

**Response:**
```json
{
  "documents": [
    {
      "id": "uuid",
      "filename": "document.txt",
      "content": "file content...",
      "file_size": 1024,
      "metadata": {},
      "created_at": "2024-01-01T00:00:00",
      "updated_at": "2024-01-01T00:00:00",
      "status": "completed"
    }
  ],
  "total": 100,
  "page": 1,
  "size": 10
}
```

#### GET /knowledge/{doc_id}
Láº¥y document theo ID.

#### PUT /knowledge/{doc_id}
Cáº­p nháº­t document.

#### DELETE /knowledge/{doc_id}
XÃ³a document.

### Chat Interface

#### POST /chat
Chat vá»›i knowledge base.

**Request:**
```json
{
  "question": "What is the main topic of the uploaded documents?",
  "chat_id": "optional-uuid"
}
```

**Response:**
```json
{
  "chat_id": "uuid",
  "response": "Based on the uploaded documents...",
  "retrieved_docs": [
    {
      "id": "uuid",
      "filename": "document.txt"
    }
  ]
}
```

#### GET /audit/{chat_id}
Láº¥y audit log cho chat session.

**Response:**
```json
{
  "chat_id": "uuid",
  "question": "What is the main topic?",
  "response": "Based on the documents...",
  "retrieved_docs": [
    {
      "id": "uuid",
      "filename": "document.txt"
    }
  ],
  "latency_ms": 450,
  "timestamp": "2024-01-01T00:00:00",
  "feedback": null,
  "model_confidence": 0.95
}
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `NEON_DATABASE_URL` | Neon PostgreSQL connection string | Required |
| `GOOGLE_API_KEY` | Google AI API key | Required |
| `REDIS_URL` | Redis connection string | `redis://redis:6379` |
| `MAX_FILE_SIZE` | Maximum file upload size (bytes) | `10485760` (10MB) |
| `CHUNK_SIZE` | Text chunk size for processing | `1000` |
| `CHUNK_OVERLAP` | Chunk overlap size | `200` |
| `LOG_LEVEL` | Logging level | `INFO` |

### File Processing

- **Supported Formats**: `.txt`, `.md`, `.csv`, `.json`
- **Max File Size**: 10MB (configurable)
- **Chunking**: Configurable chunk size vÃ  overlap
- **Async Processing**: Background tasks cho file processing

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI App   â”‚    â”‚  Neon PostgreSQLâ”‚    â”‚   Redis Cache   â”‚
â”‚                 â”‚    â”‚  (with pgvector)â”‚    â”‚   (Optional)    â”‚
â”‚  - File Upload  â”‚â—„â”€â”€â–ºâ”‚  - Documents    â”‚    â”‚  - Embeddings   â”‚
â”‚  - Chat API     â”‚    â”‚  - Audit Logs   â”‚    â”‚  - Query Cache  â”‚
â”‚  - RAG Pipeline â”‚    â”‚  - Vector Index â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Gemini API    â”‚
â”‚  - Embeddings   â”‚
â”‚  - Generation   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Performance

- **Target Latency**: < 500ms cho inputs < 500 tokens
- **Async I/O**: FastAPI async endpoints
- **Connection Pooling**: Database connection pooling
- **Vector Indexing**: HNSW index cho similarity search
- **Caching**: Redis cho frequently accessed data

## ğŸ” Monitoring

### Health Check
```bash
curl http://localhost:8000/health
```

### Logs
```bash
# Docker logs
docker-compose logs -f app

# Local logs
tail -f app.log
```

## ğŸ§ª Testing

### Sample Requests

#### Upload File
```bash
# Create test file
echo "This is a test document about AI and machine learning." > test.txt

# Upload file
curl -X POST "http://localhost:8000/knowledge/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@test.txt"
```

#### Chat with Knowledge Base
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is the main topic of the uploaded documents?"
  }'
```

#### Get Documents
```bash
curl "http://localhost:8000/knowledge?page=1&size=10"
```

## ğŸš€ Deployment

### Docker Deployment
```bash
# Production build
docker-compose -f docker-compose.yml up -d

# View logs
docker-compose logs -f
```

### Environment Setup
1. Set up Neon PostgreSQL database
2. Enable pgvector extension
3. Configure environment variables
4. Deploy with Docker

## ğŸ“ License

MIT License

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Commit changes
4. Push to branch
5. Create Pull Request # Knowledge-based-chatbot
